{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Model Partner] Package a machine learning model for listing on Vulcan\n",
    "* **conda_pytorch_p310**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram provides an overview of the ML model packaging process. In the diagram  step 1 you will store model artifacts and serving/scoring logic. In step 2 you create and push a container to ECR that is used to host your model on SageMaker which performs inference, and returns the prediction. In step 3 you validate the container can succesfully host your model on SageMaker. This notebook assumes step 1 to 3 are complete.\n",
    "\n",
    "\n",
    "**Step 4** you will learn how to package the ML model into a Model Package. In **step 5** you will validate this ML model package by deploying it with Amazon SageMaker. In **step 6** you will learn about resources that guide you on how to list the ML model in AWS Marketplace.\n",
    "\n",
    "<img src=\"images/ml-model-publishing-workflow.png\"/>\n",
    "\n",
    "\n",
    "**Table of contents**\n",
    "1. [Step 4 - Create an ML Model Package](#step4):\n",
    "    1. [Step 4.1 Define parameters](step41)\n",
    "    1. [Step 4.1 Create Model Package](step42)\n",
    "2. [Step 5 - Validate model in Amazon SageMaker environment](#step5): \n",
    "    1. [Step 5.1 Validate Real-time inference via Amazon SageMaker Endpoint](#step51)\n",
    "7. [Step 6 - List ML model on AWS Marketplace](#step6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## pre-request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Remove/comment out the following code:**\n",
    "\n",
    "\n",
    "```\n",
    "380 if report.has_errors():\n",
    "381                 raise ParamValidationError(report=report.generate_report())\n",
    "```\n",
    "\n",
    "Restart the Kernal, import boto3 again and re-run the cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strPythonPath = !which python\n",
    "strValidatePath = os.path.join(strPythonPath[0].rsplit(\"/\", 2)[0], \"lib/python3.10/site-packages/botocore/validate.py\")\n",
    "print (\"vi \" + strValidatePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**2. Grant ECR permissiomn: AmazonEC2ContainerRegistryFullAccess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "print (f'role: {sagemaker.get_execution_role()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install -U boto3\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    !{sys.executable} -m pip uninstall pycodestyle -y\n",
    "\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an ML Model Artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 model loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strPrefix = \"triton-ncf\"\n",
    "strModelName = \"ncf_food_model\"\n",
    "strTrainedModelDir = \"./custom-model\"\n",
    "strModelServingFolder = \"triton-docker-serve-pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.inference import model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 만약 prediction에 customization이 필요하다면, \"./src/model.py\"의 class NCF(nn.Module)의 forward 펑션 수정할 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncf_food_model = model_fn(strTrainedModelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncf_food_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Conversion to torchscript "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trace_model(mode, device, model, dummy_inputs, trace_model_name):\n",
    "\n",
    "    model = model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    if mode == 'trace' : IR_model = torch.jit.trace(model, dummy_inputs)\n",
    "    elif mode == 'script': IR_model = torch.jit.script(model)\n",
    "\n",
    "    print(f\"As {mode} : Model is saved {trace_model_name}\")\n",
    "    torch.jit.save(IR_model, trace_model_name)\n",
    "\n",
    "    print(\"#### Load Test ####\")    \n",
    "    loaded_m = torch.jit.load(trace_model_name)    \n",
    "    print(loaded_m.code)    \n",
    "    dummy_user = dummy_inputs[0]\n",
    "    dummy_item = dummy_inputs[1]    \n",
    "    \n",
    "    result = loaded_m(dummy_user, dummy_item)\n",
    "    print(\"Result shape: \", result.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_trace, is_script = True, False\n",
    "\n",
    "if is_trace: mode = 'trace'    \n",
    "elif is_script: mode = 'script'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "user_np = np.zeros((1,100)).astype(np.int32)\n",
    "item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "dummy_inputs = [\n",
    "    torch.from_numpy(user_np).to(device),\n",
    "    torch.from_numpy(item_np).to(device)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strTraceFoodModelName = 'ncf_food_model.pt'\n",
    "trace_model(mode, device, ncf_food_model, dummy_inputs, strTraceFoodModelName) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.Create config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ncf_food_config.pbtxt\n",
    "\n",
    "name: \"ncf_food_model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 128\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4 Artifact packaging\n",
    "- 아래와 닽은 폴더 구조를 생성해야 함.\n",
    "```\n",
    "model_serving_folder\n",
    "    - model_name\n",
    "        - version_number\n",
    "            - model file\n",
    "        - config file\n",
    "        \n",
    "# Example: \n",
    "\n",
    "triton-serve-pt\n",
    "    - ncf_food\n",
    "        - 1\n",
    "            - model.pt\n",
    "        - config.pbtxt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.triton import copy_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ncf_food_model 폴더 생성\n",
    "food_config = 'ncf_food_config.pbtxt'\n",
    "copy_artifact(strModelServingFolder, strModelName, strTraceFoodModelName, food_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Upload model packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from utils.triton import tar_artifact, upload_tar_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strModelTarFile = tar_artifact(strModelServingFolder, strModelName)    \n",
    "print(\"strModelTarFile: \", strModelTarFile)\n",
    "strModelUriPt = upload_tar_s3(sagemaker_session, strModelTarFile, strPrefix)\n",
    "print(\"strModelUriPt: \", strModelUriPt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Remove files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listFilePath = [\n",
    "    strTraceFoodModelName,\n",
    "    f'{strModelName}.model.tar.gz',\n",
    "    food_config\n",
    "]\n",
    "for strFilePath in listFilePath:\n",
    "    if os.path.exists(strFilePath):\n",
    "        os.remove(strFilePath)\n",
    "    else:\n",
    "        print(\"Can not delete the file as it doesn't exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Create custom docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ecr import ecr_handler\n",
    "from utils.triton import account_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecr = ecr_handler()\n",
    "strAccountID = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "strRegion = boto3.Session().region_name\n",
    "strBucketName = sagemaker_session.default_bucket()\n",
    "strExecutionRole = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deep learning contatiners\n",
    "    - https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "* Triton ver.\n",
    "    - 23.01, 23.02, 23.03 and 22.07\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strBase = \"amazonaws.com.cn\" if strRegion.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "strTritonImageUri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "        account_id=account_id_map[strRegion],\n",
    "        region=strRegion,\n",
    "        base=strBase\n",
    "    )\n",
    ")\n",
    "print(f'strtTritonImageUri: {strTritonImageUri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile custom-docker/Dockerfile\n",
    "\n",
    "FROM 785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n",
    "RUN pip install -U pip\n",
    "RUN pip install -U sagemaker\n",
    "RUN pip install -U boto3\n",
    "ENV PYTHONUNBUFFERED=TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 docker build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strRepositoryName=\"js-onboarding\"  ## <-- 원하는 docker repostory 이름을 추가\n",
    "strRepositoryName = strRepositoryName.lower()\n",
    "strDockerFile = \"Dockerfile\"\n",
    "strDockerDir = \"./custom-docker/\"\n",
    "strTag = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecr.build_docker(strDockerDir, strDockerFile, strRepositoryName, strRegionName=\"us-east-1\", strAccountId=\"785573368785\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Push to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strEcrRepositoryUri = ecr.register_image_to_ecr(strRegion, strAccountID, strRepositoryName, strTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#strEcrRepositoryUri = \"419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest\"\n",
    "print(f'strEcrRepositoryUri: {strEcrRepositoryUri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Validation (Serving and Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = True\n",
    "\n",
    "if local_mode:\n",
    "    \n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    strInstanceType = \"local_gpu\"\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    \n",
    "else:\n",
    "    strInstanceType = \"ml.m5.2xlarge\" #\"ml.p3.2xlarge\"#\"ml.g4dn.8xlarge\"#\"ml.p3.2xlarge\", 'ml.p3.16xlarge' , ml.g4dn.8xlarge\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "\n",
    "nInstanceCount = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Local mode\n",
    "- 내부적으로 Triton 서버가 구동시에 아래 URL 스크립트가 구동 됨.\n",
    "    - 여기에 맞는 필요한 환경 변수를 넣어 줌.\n",
    "    - https://raw.githubusercontent.com/triton-inference-server/server/main/docker/sagemaker/serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Depoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from sagemaker.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# endpoint variables\n",
    "strSMModelName = f\"{strPrefix}-mdl-{ts}\" #sm_model_name\n",
    "strEndpointConfigName = f\"{strPrefix}-epc-{ts}\" # endpoint_config_name\n",
    "strEndpointName = f\"{strPrefix}-ep-{ts}\" # endpoint_name\n",
    "strModelDataUrl = f\"s3://{strBucketName}/{strPrefix}/\" #model_data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicContainerEnvs = {\n",
    "                    \"SAGEMAKER_TRITON_LOG_VERBOSE\": \"3\",\n",
    "                    \"SAGEMAKER_TRITON_LOG_INFO\": \"1\",\n",
    "                    \"SAGEMAKER_TRITON_LOG_WARNING\" : \"1\",\n",
    "                    \"SAGEMAKER_TRITON_LOG_ERROR\" : \"1\"\n",
    "                 }\n",
    "\n",
    "localPytorchModel = Model(\n",
    "    model_data= strModelUriPt,\n",
    "    image_uri = strEcrRepositoryUri,\n",
    "    role=strExecutionRole,\n",
    "    env = dicContainerEnvs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "localPredictor = localPytorchModel.deploy(\n",
    "    instance_type=strInstanceType,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=strEndpointName,\n",
    "    wait=True,\n",
    "    log=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sample_payload():\n",
    "    # user\n",
    "    user_np = np.zeros((1,100)).astype(np.int32)\n",
    "    # item\n",
    "    item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": \"INPUT__0\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": user_np.tolist()},\n",
    "            {\"name\": \"INPUT__1\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": item_np.tolist()},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return payload\n",
    "\n",
    "payload = create_sample_payload()\n",
    "print(\"payload: \", payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_model_invoke_endpoint(client,endpoint_name, payload): \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/octet-stream\", \n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "\n",
    "    result = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "    \n",
    "    return result\n",
    "\n",
    "runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()    \n",
    "result = single_model_invoke_endpoint(runtime_client,strEndpointName, payload)\n",
    "print(\"result : \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.inference_utils import delete_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = sagemaker.local.LocalSagemakerClient()\n",
    "delete_endpoint(client, strEndpointName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cloud mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**local_mode = True 변경하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Depoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicContainer = {\n",
    "    \"Image\": strEcrRepositoryUri,\n",
    "    \"ModelDataUrl\": strModelUriPt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'dicContainer: {dicContainer}')\n",
    "print(f'strSMModelName: {strSMModelName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=strSMModelName,\n",
    "    ExecutionRoleArn=strExecutionRole,\n",
    "    PrimaryContainer=dicContainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Model Arn: {create_model_response[\"ModelArn\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=strEndpointConfigName,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": strInstanceType,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": strSMModelName,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Endpoint Config Arn: {create_endpoint_config_response[\"EndpointConfigArn\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=strEndpointName,\n",
    "    EndpointConfigName=strEndpointConfigName\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Endpoint Arn: {create_endpoint_response[\"EndpointArn\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=strEndpointName)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=strEndpointName)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "single_model_invoke_endpoint(runtime_client,strEndpointName, payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.Session().client('sagemaker')\n",
    "delete_endpoint(client, strEndpointName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create an ML Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role, ModelPackage\n",
    "import time\n",
    "\n",
    "# Common variables\n",
    "session = sage.Session()\n",
    "s3_bucket = session.default_bucket()\n",
    "region = session.boto_region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_client = session.boto_session.client(\"s3\")\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see how you can package your artifacts (ECR image and the trained model artifacts) into a ModelPackage. Once you complete this, you can list your product as a pretrained model in the AWS Marketplace.\n",
    "\n",
    "**NOTE:** If your model can be deployed on multiple hardware types (CPU/GPU/Inferentia) then a ModelPackage must be created for each and added to the MP listing as different versions as, in general, the container image used will be different for each.  \n",
    "\n",
    "#### Model Package Definition\n",
    "A Model Package is a reusable abstraction for model artifacts that packages all the ingredients necessary for inference. It consists of an inference specification that defines the inference image to use along with an optional model data location.\n",
    "\n",
    "The ModelPackage must be created in the AWS account that will be registered as a seller on the AWS Marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "model_name = \"marketplace-model-test\"#\"<<YourModelName>>\"\n",
    "model_description = \"marketplace-model-test\"#\"<<YourModelDescription>>\"\n",
    "\n",
    "# <<YourSupportedContentTypes>>\n",
    "supported_content_types = [\"application/octet-stream\"]#[\"text/csv\", \"application/json\", \"application/jsonlines\"]\n",
    "\n",
    "# <<YourSupportedResponseMIMETypes>>\n",
    "supported_response_MIME_types = [ \n",
    "    \"application/json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Model Package creation process requires you to specify following:\n",
    "  1. Docker image\n",
    "  2. Model artifacts\n",
    "    - You can either package these inside the docker image, as we have done in this example, or provide them as a gzipped tarball.\n",
    "    - In the case of large Models gzipped tarball is required. \n",
    "  3. Validation specification \n",
    "        \n",
    "In order to provide confidence to sellers (and buyers) that the products work in Amazon SageMaker, before listing them on AWS Marketplace SageMaker needs to perform basic validations. The product can be listed in AWS Marketplace only if this validation process succeeds. This validation process uses the validation profile and sample data provided by you to create a transform job in your account using the Model to verify your inference image works with SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to identify the right instance-sizes for your ML models. You can do so by running performance tests on top of your ML Model.\n",
    "\n",
    "**NOTE:** In addition to tuning, take into account the requirements of your model when identifying instance types.  If your model does not use GPU resources, then do not include GPU instance types.  Similarly, if your model does use GPU resources, but can only make use of a single GPU, do not include instance types that have multiple GPUs as it will lead to increased infrastructure charges for your customers with no performance benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "supported_realtime_inference_instance_types = [\"ml.g5.12xlarge\"]#[\"<<YourModelSupportInstanceType>>\"]\n",
    "supported_batch_transform_instance_types = [\"ml.m5.xlarge\"] #  use either a g4dn.12xlarge or p3.8xlarge. However, the Batch Transform validation step is not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_file_name = \"input.jsonl\"\n",
    "validation_input_path = f\"s3://{s3_bucket}/validation-input-json/\"\n",
    "validation_output_path = f\"s3://{s3_bucket}/validation-output-jsonl/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Not required)* First, we create sample data to be used in the validation stage of the ModelPackage creation and upload it to S3. This sample data would need to be in the format your model expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_line = json.dumps(payload)\n",
    "with open(\"input.jsonl\", \"w\") as f:\n",
    "    f.write(json_line)\n",
    "s3_client.put_object(Bucket=s3_bucket, Key=\"validation-input-json/input.jsonl\", Body=json_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (strEcrRepositoryUri)\n",
    "print (strModelUriPt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Model Package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#docker_image_uri = \"419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest\"\n",
    "#model_data_location = \"s3://sagemaker-us-east-1-419974056037/triton-ncf/ncf_food_model.model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docker_image_uri = strEcrRepositoryUri#\"<<YourModelImageURI>>\" # ECR URI of Image used to host model\n",
    "model_data_location = strModelUriPt#\"<<YourModelS3Location>>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating the ModelPackage you will recieve the error:\n",
    "\n",
    "```\n",
    "~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/validate.py in serialize_to_request(self, parameters, operation_model)\n",
    "    380             if report.has_errors():\n",
    "--> 381                raise ParamValidationError(report=report.generate_report())\n",
    "    382         return self._serializer.serialize_to_request(\n",
    "    383             parameters, operation_model\n",
    "\n",
    "ParamValidationError: Parameter validation failed:\n",
    "Invalid length for parameter ValidationSpecification.ValidationProfiles, value: 0, valid min length: 1\n",
    "```\n",
    "\n",
    "In order to resolve this issue, open the `validate.py` file in this case it is located at `~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/validate.py`. Remove/comment out the following code: \n",
    "\n",
    "```\n",
    "380 if report.has_errors():\n",
    "381                 raise ParamValidationError(report=report.generate_report())\n",
    "```\n",
    "\n",
    "Restart the Kernal, import boto3 again and re-run the cell. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!which python** <BR>\n",
    "**for pytorch_p310: cd ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/validate.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package = session.sagemaker_client.create_model_package(\n",
    "    ModelPackageName=model_name,\n",
    "    ModelPackageDescription=model_description,\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": docker_image_uri,\n",
    "                \"ModelDataUrl\": model_data_location\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedTransformInstanceTypes\": supported_batch_transform_instance_types,\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": supported_realtime_inference_instance_types,\n",
    "        \"SupportedContentTypes\": supported_content_types,\n",
    "        \"SupportedResponseMIMETypes\": supported_response_MIME_types,\n",
    "    },\n",
    "    CertifyForMarketplace=True,  # Make sure to set this to True\n",
    "    #ValidationSpecification={\n",
    "    #    \"ValidationRole\": role,\n",
    "    #    \"ValidationProfiles\": [],\n",
    "    #},\n",
    "    ValidationSpecification={\n",
    "        'ValidationRole': role,\n",
    "        'ValidationProfiles': [\n",
    "            {\n",
    "                'ProfileName': \"validation\",\n",
    "                'TransformJobDefinition': {\n",
    "                    'MaxConcurrentTransforms': 1,\n",
    "                    'MaxPayloadInMB': 64,\n",
    "                    'BatchStrategy': 'SingleRecord',\n",
    "                    #'Environment': {\n",
    "                    #    'string': 'string'\n",
    "                    #},\n",
    "                    'TransformInput': {\n",
    "                        'DataSource': {\n",
    "                            'S3DataSource': {\n",
    "                                'S3DataType': 'S3Prefix',\n",
    "                                'S3Uri': f'{validation_input_path}input.jsonl'\n",
    "                            }\n",
    "                        },\n",
    "                        'ContentType': 'application/octet-stream',\n",
    "                        'CompressionType': 'None',\n",
    "                        'SplitType': 'None'\n",
    "                    },\n",
    "                    'TransformOutput': {\n",
    "                        'S3OutputPath': f'{validation_output_path}output.json',\n",
    "                        'Accept': 'application/json',\n",
    "                        'AssembleWith': 'None',\n",
    "                        #'KmsKeyId': 'string'\n",
    "                    },\n",
    "                    'TransformResources': {\n",
    "                        'InstanceType': 'ml.m5.xlarge',\n",
    "                        'InstanceCount': 1,\n",
    "                        #'VolumeKmsKeyId': 'string'\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.wait_for_model_package(model_package_name=model_name) # If failure occurs navigate to SageMaker Console > My marketplace model packages > select the failed ModelPackage for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the preceding cell, open the [Model Packages console from Amazon SageMaker](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources) and check if model creation succeeded. \n",
    "\n",
    "Choose the Model and then open the **Validation** tab to see the validation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate model in Amazon SageMaker environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a deployable model from the model package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package[\"ModelPackageArn\"],\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Validate Real-time inference via Amazon SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy the SageMaker model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=supported_realtime_inference_instance_types[0],\n",
    "    endpoint_name=model_name,\n",
    ")\n",
    "model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type = supported_content_types[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example invocation via boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make use of your own example input data to test the Endpoint\n",
    "#input_json = '{\"text\": \"sample\"}'\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=model.endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "json.load(response[\"Body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example invocation via the AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "    --endpoint-name $model.endpoint_name \\\n",
    "    --body fileb://$validation_file_name \\\n",
    "    --content-type $content_type \\\n",
    "    --region $session.boto_region_name \\\n",
    "    out.out\n",
    "    \n",
    "    \n",
    "# Print inference\n",
    "!head out.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the endpoint and endpoint configuration created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model.endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Since the model is not required, you can delete it. Note that you are deleting the deployable model. Not the model package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To publish the model to the AWS Marketplace, you will need to specify model package ARN. Copy the following Model Package ARN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package[\"ModelPackageArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"step6\"></a>Step 6: List ML Model on AWS Marketplace\n",
    "\n",
    "\n",
    "1. Model Partner creates [public profile](https://docs.aws.amazon.com/marketplace/latest/userguide/seller-registration-process.html#seller-public-profile) on AWS Marketplace and registers to be a seller.\n",
    "There is no need to provide Tax information as the product on Marketplace will be listed as free.\n",
    "\n",
    "2. In the [Model Packages](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources) section of the SageMaker console you'll find the entity you created in this notebook. If it was successfully created and validated, you should be able to select the entity and choose **Publish new ML Marketplace listing**.\n",
    "\n",
    "<img src=\"images/publish-to-marketplace-action.png\"/>\n",
    "\n",
    "You will be redirected to the [AWS Marketplace Management portal](https://aws.amazon.com/marketplace/management/ml-products/) where you will be able to build a listing.\n",
    "\n",
    "<img src=\"images/listing.png\"/>\n",
    "\n",
    "1. If your model targets multiple hardware types, remember to add each ModelPackage to the listing as separate versions.\n",
    "2. Click Add and fill in the model information. Kindly set Product visibility must be set to `Public`.\n",
    "<img src=\"images/public.png\"/>\n",
    "\n",
    "3. Allowlist account `171503325295`, `572320329544` and `559110549532` for access to the model. \n",
    "For region support select: `us-east-1, us-west-2, eu-west-1, eu-central-1, eu-west-2, ap-northeast-1, ap-south-1, ca-central-1, us-east-2, ap-northeast-2`\n",
    "<img src=\"images/allowlist-accs.png\"/>\n",
    "\n",
    "4. Under Pricing and terms, set pricing model as:\n",
    "**Inference based pricing (custom metering) at $0**\n",
    "\n",
    "You will see the following:\n",
    "(Optional) If the container did not implement the below please confirm and move forward. \n",
    "```\n",
    "I confirm that my model package supports the response header for custom metering. Example response header: X-Amzn-Inference-Metering:\n",
    "{\"Dimension\": \"inference.count\", \"ConsumedUnits\": 3}\n",
    "I understand that in absence of this header, default metering will be used instead.\n",
    "```\n",
    "\n",
    "<img src=\"images/inference-based-pricing.png\"/>\n",
    "\n",
    "5. Listing status should show as follows:\n",
    "**Do not click Sign off and publish**\n",
    "\n",
    "<img src=\"images/status-1.png\"/>\n",
    "\n",
    "6. Vissibility status of the listing should be `Limited`.\n",
    "\n",
    "<img src=\"images/status-2.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "* [Publishing your product in AWS Marketplace](https://docs.aws.amazon.com/marketplace/latest/userguide/ml-publishing-your-product-in-aws-marketplace.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
