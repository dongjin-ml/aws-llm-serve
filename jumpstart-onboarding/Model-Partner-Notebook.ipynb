{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Model Partner] Package a machine learning model for listing on Vulcan\n",
    "* conda_pytorch_p310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram provides an overview of the ML model packaging process. In the diagram  step 1 you will store model artifacts and serving/scoring logic. In step 2 you create and push a container to ECR that is used to host your model on SageMaker which performs inference, and returns the prediction. In step 3 you validate the container can succesfully host your model on SageMaker. This notebook assumes step 1 to 3 are complete.\n",
    "\n",
    "\n",
    "**Step 4** you will learn how to package the ML model into a Model Package. In **step 5** you will validate this ML model package by deploying it with Amazon SageMaker. In **step 6** you will learn about resources that guide you on how to list the ML model in AWS Marketplace.\n",
    "\n",
    "<img src=\"images/ml-model-publishing-workflow.png\"/>\n",
    "\n",
    "\n",
    "**Table of contents**\n",
    "1. [Step 4 - Create an ML Model Package](#step4):\n",
    "    1. [Step 4.1 Define parameters](step41)\n",
    "    1. [Step 4.1 Create Model Package](step42)\n",
    "2. [Step 5 - Validate model in Amazon SageMaker environment](#step5): \n",
    "    1. [Step 5.1 Validate Real-time inference via Amazon SageMaker Endpoint](#step51)\n",
    "7. [Step 6 - List ML model on AWS Marketplace](#step6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once\n",
    "# install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already revised\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "DAEMON_PATH=\"/etc/docker\"\n",
    "MEMORY_SIZE=10G\n",
    "\n",
    "FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# echo $FLAG\n",
    "\n",
    "if [ \"$FLAG\" == true ]; then\n",
    "    echo \"Already revised\"\n",
    "else\n",
    "    echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "    sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "    sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "    sudo service docker restart\n",
    "    echo \"Docker Restart\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing deps and restarting kernel\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (23.1.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.26.150)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.26.162-py3-none-any.whl (135 kB)\n",
      "Collecting botocore<1.30.0,>=1.29.162 (from boto3)\n",
      "  Using cached botocore-1.29.162-py3-none-any.whl (11.0 MB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.162->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.162->boto3) (1.26.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.162->boto3) (1.16.0)\n",
      "Installing collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.150\n",
      "    Uninstalling botocore-1.29.150:\n",
      "      Successfully uninstalled botocore-1.29.150\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.150\n",
      "    Uninstalling boto3-1.26.150:\n",
      "      Successfully uninstalled boto3-1.26.150\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.150 requires botocore==1.29.150, but you have botocore 1.29.162 which is incompatible.\n",
      "awscli 1.27.150 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.162 botocore-1.29.162\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.168.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.26.162)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.24.3)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.5.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.162 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.29.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (67.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.162->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "\u001b[33mWARNING: Skipping pycodestyle as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install -U boto3\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    !{sys.executable} -m pip uninstall pycodestyle -y\n",
    "\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an ML Model Artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 model loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strPrefix = \"triton-ncf\"\n",
    "strModelName = \"ncf_food_model\"\n",
    "strTrainedModelDir = \"./custom-model\"\n",
    "strModelServingFolder = \"triton-docker-serve-pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.inference import model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 만약 prediction에 customization이 필요하다면, \"./src/model.py\"의 class NCF(nn.Module)의 forward 펑션 수정할 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Staring model_fn() ###############\n",
      "--> model_dir : ./custom-model\n",
      "model_config_path: :  /home/ec2-user/SageMaker/aws-llm-serve/jumpstart-onboarding/src/model_config.json\n",
      "--> model network is loaded\n",
      "model_file_path: :  ./custom-model/NeuMF-end.pth\n",
      "####### Model is loaded #########\n"
     ]
    }
   ],
   "source": [
    "ncf_food_model = model_fn(strTrainedModelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCF(\n",
       "  (embed_user_GMF): Embedding(6040, 32)\n",
       "  (embed_item_GMF): Embedding(3706, 32)\n",
       "  (embed_user_MLP): Embedding(6040, 128)\n",
       "  (embed_item_MLP): Embedding(3706, 128)\n",
       "  (linear): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (MLP_layers): Sequential(\n",
       "    (0): Dropout(p=0.0, inplace=False)\n",
       "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.0, inplace=False)\n",
       "    (7): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (predict_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncf_food_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Conversion to torchscript "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trace_model(mode, device, model, dummy_inputs, trace_model_name):\n",
    "\n",
    "    model = model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    if mode == 'trace' : IR_model = torch.jit.trace(model, dummy_inputs)\n",
    "    elif mode == 'script': IR_model = torch.jit.script(model)\n",
    "\n",
    "    print(f\"As {mode} : Model is saved {trace_model_name}\")\n",
    "    torch.jit.save(IR_model, trace_model_name)\n",
    "\n",
    "    print(\"#### Load Test ####\")    \n",
    "    loaded_m = torch.jit.load(trace_model_name)    \n",
    "    print(loaded_m.code)    \n",
    "    dummy_user = dummy_inputs[0]\n",
    "    dummy_item = dummy_inputs[1]    \n",
    "    \n",
    "    result = loaded_m(dummy_user, dummy_item)\n",
    "    print(\"Result shape: \", result.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "is_trace, is_script = True, False\n",
    "\n",
    "if is_trace: mode = 'trace'    \n",
    "elif is_script: mode = 'script'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "user_np = np.zeros((1,100)).astype(np.int32)\n",
    "item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "dummy_inputs = [\n",
    "    torch.from_numpy(user_np).to(device),\n",
    "    torch.from_numpy(item_np).to(device)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As trace : Model is saved ncf_food_model.pt\n",
      "#### Load Test ####\n",
      "def forward(self,\n",
      "    user: Tensor,\n",
      "    item: Tensor) -> Tensor:\n",
      "  predict_layer = self.predict_layer\n",
      "  MLP_layers = self.MLP_layers\n",
      "  embed_item_MLP = self.embed_item_MLP\n",
      "  embed_user_MLP = self.embed_user_MLP\n",
      "  embed_item_GMF = self.embed_item_GMF\n",
      "  embed_user_GMF = self.embed_user_GMF\n",
      "  output_GMF = torch.mul((embed_user_GMF).forward(user, ), (embed_item_GMF).forward(item, ))\n",
      "  _0 = [(embed_user_MLP).forward(user, ), (embed_item_MLP).forward(item, )]\n",
      "  input = torch.cat(_0, -1)\n",
      "  _1 = [output_GMF, (MLP_layers).forward(input, )]\n",
      "  input0 = torch.cat(_1, -1)\n",
      "  return (predict_layer).forward(input0, )\n",
      "\n",
      "Result shape:  torch.Size([1, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "strTraceFoodModelName = 'ncf_food_model.pt'\n",
    "trace_model(mode, device, ncf_food_model, dummy_inputs, strTraceFoodModelName) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.Create config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ncf_food_config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ncf_food_config.pbtxt\n",
    "\n",
    "name: \"ncf_food_model\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 128\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [100]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4 Artifact packaging\n",
    "- 아래와 닽은 폴더 구조를 생성해야 함.\n",
    "```\n",
    "model_serving_folder\n",
    "    - model_name\n",
    "        - version_number\n",
    "            - model file\n",
    "        - config file\n",
    "        \n",
    "# Example: \n",
    "\n",
    "triton-serve-pt\n",
    "    - ncf_food\n",
    "        - 1\n",
    "            - model.pt\n",
    "        - config.pbtxt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.triton import copy_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton-docker-serve-pt:\n",
      "ncf_food_model\n",
      "\n",
      "triton-docker-serve-pt/ncf_food_model:\n",
      "1\n",
      "config.pbtxt\n",
      "\n",
      "triton-docker-serve-pt/ncf_food_model/1:\n",
      "model.pt\n"
     ]
    }
   ],
   "source": [
    "# ncf_food_model 폴더 생성\n",
    "food_config = 'ncf_food_config.pbtxt'\n",
    "copy_artifact(strModelServingFolder, strModelName, strTraceFoodModelName, food_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Upload model packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from utils.triton import tar_artifact, upload_tar_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxr-x ec2-user/ec2-user 0 2023-06-28 02:36 ncf_food_model/\n",
      "-rw-rw-r-- ec2-user/ec2-user 307 2023-06-28 02:36 ncf_food_model/config.pbtxt\n",
      "drwxrwxr-x ec2-user/ec2-user   0 2023-06-28 02:36 ncf_food_model/1/\n",
      "-rw-rw-r-- ec2-user/ec2-user 6444775 2023-06-28 02:36 ncf_food_model/1/model.pt\n",
      "strModelTarFile:  ncf_food_model.model.tar.gz\n",
      "strModelUriPt:  s3://sagemaker-us-east-1-419974056037/triton-ncf/ncf_food_model.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "strModelTarFile = tar_artifact(strModelServingFolder, strModelName)    \n",
    "print(\"strModelTarFile: \", strModelTarFile)\n",
    "strModelUriPt = upload_tar_s3(sagemaker_session, strModelTarFile, strPrefix)\n",
    "print(\"strModelUriPt: \", strModelUriPt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Remove files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listFilePath = [\n",
    "    strTraceFoodModelName,\n",
    "    f'{strModelName}.model.tar.gz',\n",
    "    food_config\n",
    "]\n",
    "for strFilePath in listFilePath:\n",
    "    if os.path.exists(strFilePath):\n",
    "        os.remove(strFilePath)\n",
    "    else:\n",
    "        print(\"Can not delete the file as it doesn't exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Create custom docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ecr import ecr_handler\n",
    "from utils.triton import account_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecr = ecr_handler()\n",
    "strAccountID = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "strRegion = boto3.Session().region_name\n",
    "strBucketName = sagemaker_session.default_bucket()\n",
    "strExecutionRole = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deep learning contatiners\n",
    "    - https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "* Triton ver.\n",
    "    - 23.01, 23.02, 23.03 and 22.07\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strtTritonImageUri: 785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n"
     ]
    }
   ],
   "source": [
    "strBase = \"amazonaws.com.cn\" if strRegion.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "strTritonImageUri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "        account_id=account_id_map[strRegion],\n",
    "        region=strRegion,\n",
    "        base=strBase\n",
    "    )\n",
    ")\n",
    "print(f'strtTritonImageUri: {strTritonImageUri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom-docker/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom-docker/Dockerfile\n",
    "\n",
    "FROM 785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n",
    "RUN pip install -U pip\n",
    "RUN pip install -U sagemaker\n",
    "RUN pip install -U boto3\n",
    "ENV PYTHONUNBUFFERED=TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 docker build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strRepositoryName=\"js-onboarding\"  ## <-- 원하는 docker repostory 이름을 추가\n",
    "strRepositoryName = strRepositoryName.lower()\n",
    "strDockerFile = \"Dockerfile\"\n",
    "strDockerDir = \"./custom-docker/\"\n",
    "strTag = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/aws-llm-serve/jumpstart-onboarding\n",
      "/home/ec2-user/SageMaker/aws-llm-serve/jumpstart-onboarding/custom-docker\n",
      "strDockerFile Dockerfile\n",
      "aws ecr get-login --region 'us-east-1' --registry-ids '785573368785' --no-include-email\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "\n",
      "Step 1/5 : FROM 785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n",
      " ---> 455241bf5ae4\n",
      "Step 2/5 : RUN pip install -U pip\n",
      " ---> Running in aa7a0cff3829\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 56.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.2\n",
      "    Uninstalling pip-22.2.2:\n",
      "      Successfully uninstalled pip-22.2.2\n",
      "Successfully installed pip-23.1.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container aa7a0cff3829\n",
      " ---> 2d8b7bf59cad\n",
      "Step 3/5 : RUN pip install -U sagemaker\n",
      " ---> Running in b5b562ee1bec\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.168.0.tar.gz (844 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 844.7/844.7 kB 52.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting attrs<24,>=23.1.0 (from sagemaker)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 12.2 MB/s eta 0:00:00\n",
      "Collecting boto3<2.0,>=1.26.131 (from sagemaker)\n",
      "  Downloading boto3-1.26.162-py3-none-any.whl (135 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.9/135.9 kB 25.8 MB/s eta 0:00:00\n",
      "Collecting cloudpickle==2.2.1 (from sagemaker)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting google-pasta (from sagemaker)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 13.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from sagemaker) (1.23.1)\n",
      "Collecting protobuf<4.0,>=3.1 (from sagemaker)\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 89.2 MB/s eta 0:00:00\n",
      "Collecting protobuf3-to-dict<1.0,>=0.1.5 (from sagemaker)\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting smdebug_rulesconfig==1.0.1 (from sagemaker)\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting importlib-metadata<5.0,>=1.4.0 (from sagemaker)\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Collecting packaging>=20.0 (from sagemaker)\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting pandas (from sagemaker)\n",
      "  Downloading pandas-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 115.2 MB/s eta 0:00:00\n",
      "Collecting pathos (from sagemaker)\n",
      "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.8/79.8 kB 19.1 MB/s eta 0:00:00\n",
      "Collecting schema (from sagemaker)\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Collecting PyYAML==6.0 (from sagemaker)\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 701.2/701.2 kB 66.4 MB/s eta 0:00:00\n",
      "Collecting jsonschema (from sagemaker)\n",
      "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.4/90.4 kB 19.1 MB/s eta 0:00:00\n",
      "Collecting platformdirs (from sagemaker)\n",
      "  Downloading platformdirs-3.8.0-py3-none-any.whl (16 kB)\n",
      "Collecting tblib==1.7.0 (from sagemaker)\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting botocore<1.30.0,>=1.29.162 (from boto3<2.0,>=1.26.131->sagemaker)\n",
      "  Downloading botocore-1.29.162-py3-none-any.whl (11.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/11.0 MB 88.2 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.26.131->sagemaker)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3<2.0,>=1.26.131->sagemaker)\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.8/79.8 kB 19.9 MB/s eta 0:00:00\n",
      "Collecting zipp>=0.5 (from importlib-metadata<5.0,>=1.4.0->sagemaker)\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.14.0)\n",
      "Collecting importlib-resources>=1.4.0 (from jsonschema->sagemaker)\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema->sagemaker)\n",
      "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 (from jsonschema->sagemaker)\n",
      "  Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 14.5 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->sagemaker)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 38.1 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1 (from pandas->sagemaker)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.3/502.3 kB 53.6 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.1 (from pandas->sagemaker)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.8/341.8 kB 33.4 MB/s eta 0:00:00\n",
      "Collecting ppft>=1.7.6.6 (from pathos->sagemaker)\n",
      "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 11.5 MB/s eta 0:00:00\n",
      "Collecting dill>=0.3.6 (from pathos->sagemaker)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 22.4 MB/s eta 0:00:00\n",
      "Collecting pox>=0.3.2 (from pathos->sagemaker)\n",
      "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
      "Collecting multiprocess>=0.70.14 (from pathos->sagemaker)\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.0/132.0 kB 31.7 MB/s eta 0:00:00\n",
      "Collecting contextlib2>=0.5.5 (from schema->sagemaker)\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore<1.30.0,>=1.29.162->boto3<2.0,>=1.26.131->sagemaker) (1.25.8)\n",
      "Building wheels for collected packages: sagemaker, protobuf3-to-dict\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.168.0-py2.py3-none-any.whl size=1151020 sha256=9dcc8517d5c4f2e6b3132259aceadf5940bf44e69ed582d93dbfca824711852e\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/41/93/9f571f7fbd2808c836170e0d596b90baafece77459863b3e86\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4015 sha256=f162bfed0facec0b61c8a0afa952169e7641378a4e938f1291c6e093a7e63e86\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/10/27/2d1e23d8b9a9013a83fbb418a0b17b1e6f81c8db8f53b53934\n",
      "Successfully built sagemaker protobuf3-to-dict\n",
      "Installing collected packages: pytz, zipp, tzdata, tblib, smdebug_rulesconfig, PyYAML, python-dateutil, pyrsistent, protobuf, ppft, pox, platformdirs, pkgutil-resolve-name, packaging, jmespath, google-pasta, dill, contextlib2, cloudpickle, attrs, schema, protobuf3-to-dict, pandas, multiprocess, importlib-resources, importlib-metadata, botocore, s3transfer, pathos, jsonschema, boto3, sagemaker\n",
      "Successfully installed PyYAML-6.0 attrs-23.1.0 boto3-1.26.162 botocore-1.29.162 cloudpickle-2.2.1 contextlib2-21.6.0 dill-0.3.6 google-pasta-0.2.0 importlib-metadata-4.13.0 importlib-resources-5.12.0 jmespath-1.0.1 jsonschema-4.17.3 multiprocess-0.70.14 packaging-23.1 pandas-2.0.2 pathos-0.3.0 pkgutil-resolve-name-1.3.10 platformdirs-3.8.0 pox-0.3.2 ppft-1.7.6.6 protobuf-3.20.3 protobuf3-to-dict-0.1.5 pyrsistent-0.19.3 python-dateutil-2.8.2 pytz-2023.3 s3transfer-0.6.1 sagemaker-2.168.0 schema-0.7.5 smdebug_rulesconfig-1.0.1 tblib-1.7.0 tzdata-2023.3 zipp-3.15.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container b5b562ee1bec\n",
      " ---> 351f84f64ce1\n",
      "Step 4/5 : RUN pip install -U boto3\n",
      " ---> Running in c709b8fe4606\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (1.26.162)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.162 in /usr/local/lib/python3.8/dist-packages (from boto3) (1.29.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.162->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore<1.30.0,>=1.29.162->boto3) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.162->boto3) (1.14.0)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container c709b8fe4606\n",
      " ---> c649fed3518f\n",
      "Step 5/5 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in b4625b9897be\n",
      "Removing intermediate container b4625b9897be\n",
      " ---> d4745ab00bd1\n",
      "Successfully built d4745ab00bd1\n",
      "Successfully tagged js-onboarding:latest\n",
      "\n",
      "/home/ec2-user/SageMaker/aws-llm-serve/jumpstart-onboarding\n"
     ]
    }
   ],
   "source": [
    "ecr.build_docker(strDockerDir, strDockerFile, strRepositoryName, strRegionName=\"us-east-1\", strAccountId=\"785573368785\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Push to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== REGISTER AN IMAGE TO ECR ==\n",
      "  processing_repository_uri: 419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest\n",
      "aws ecr get-login --region 'us-east-1' --registry-ids '419974056037' --no-include-email\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "\n",
      "aws ecr create-repository --repository-name 'js-onboarding'\n",
      "docker tag 'js-onboarding:latest' '419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest'\n",
      "docker push '419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest'\n",
      "== REGISTER AN IMAGE TO ECR ==\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "strEcrRepositoryUri = ecr.register_image_to_ecr(strRegion, strAccountID, strRepositoryName, strTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strEcrRepositoryUri: 419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest\n"
     ]
    }
   ],
   "source": [
    "strEcrRepositoryUri = \"419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest\"\n",
    "print(f'strEcrRepositoryUri: {strEcrRepositoryUri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Validation (Serving and Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True to enable SageMaker to run locally\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    \n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    strInstanceType = \"local_gpu\"\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    \n",
    "else:\n",
    "    strInstanceType = \"ml.m5.2xlarge\" #\"ml.p3.2xlarge\"#\"ml.g4dn.8xlarge\"#\"ml.p3.2xlarge\", 'ml.p3.16xlarge' , ml.g4dn.8xlarge\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "\n",
    "nInstanceCount = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Local mode\n",
    "- 내부적으로 Triton 서버가 구동시에 아래 URL 스크립트가 구동 됨.\n",
    "    - 여기에 맞는 필요한 환경 변수를 넣어 줌.\n",
    "    - https://raw.githubusercontent.com/triton-inference-server/server/main/docker/sagemaker/serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Depoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from sagemaker.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# endpoint variables\n",
    "strSMModelName = f\"{strPrefix}-mdl-{ts}\" #sm_model_name\n",
    "strEndpointConfigName = f\"{strPrefix}-epc-{ts}\" # endpoint_config_name\n",
    "strEndpointName = f\"{strPrefix}-ep-{ts}\" # endpoint_name\n",
    "strModelDataUrl = f\"s3://{strBucketName}/{strPrefix}/\" #model_data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicContainerEnvs = {\n",
    "                    \"SAGEMAKER_TRITON_LOG_VERBOSE\": \"3\",\n",
    "                    \"SAGEMAKER_TRITON_LOG_INFO\": \"1\",\n",
    "                    \"SAGEMAKER_TRITON_LOG_WARNING\" : \"1\",\n",
    "                    \"SAGEMAKER_TRITON_LOG_ERROR\" : \"1\"\n",
    "                 }\n",
    "\n",
    "localPytorchModel = Model(\n",
    "    model_data= strModelUriPt,\n",
    "    image_uri = strEcrRepositoryUri,\n",
    "    role=strExecutionRole,\n",
    "    env = dicContainerEnvs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff49ed8e260>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff49ed8e3e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff49ed8e6e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "Attaching to usbfufy3dj-algo-1-stjkc\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m =============================\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m == Triton Inference Server ==\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m =============================\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m NVIDIA Release 22.07 (build <unknown>)\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m Triton Server Version 2.24.0\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m By pulling and using the container, you accept the terms and conditions of this license:\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m WARNING: No SAGEMAKER_TRITON_DEFAULT_MODEL_NAME provided.\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m          Starting with the only existing model directory ncf_food_model\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.380105 86 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f3018000000' with size 268435456\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.382253 86 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.382270 86 cuda_memory_manager.cc:105] CUDA memory pool is created on device 1 with size 67108864\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.382274 86 cuda_memory_manager.cc:105] CUDA memory pool is created on device 2 with size 67108864\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.382277 86 cuda_memory_manager.cc:105] CUDA memory pool is created on device 3 with size 67108864\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m W0628 03:05:06.587307 86 server.cc:216] failed to enable peer access for some device pairs\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.590469 86 model_config_utils.cc:645] Server side auto-completed config: name: \"ncf_food_model\"\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m platform: \"pytorch_libtorch\"\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m max_batch_size: 128\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m input {\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m   name: \"INPUT__0\"\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m   data_type: TYPE_INT32\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m   dims: 100\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m }\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m input {\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m   name: \"INPUT__1\"\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m   data_type: TYPE_INT32\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m   dims: 100\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m }\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m output {\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m   name: \"OUTPUT__0\"\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m   data_type: TYPE_FP32\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m   dims: -1\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m }\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m default_model_filename: \"model.pt\"\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m backend: \"pytorch\"\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.592438 86 model_repository_manager.cc:913] AsyncLoad() 'ncf_food_model'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.592479 86 model_repository_manager.cc:1151] TriggerNextAction() 'ncf_food_model' version 1: 1\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.592495 86 model_repository_manager.cc:1187] Load() 'ncf_food_model' version 1\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.592499 86 model_repository_manager.cc:1206] loading: ncf_food_model:1\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.592556 86 model_repository_manager.cc:1256] CreateModel() 'ncf_food_model' version 1\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.592631 86 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.592657 86 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/pytorch/libtriton_pytorch.so\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.920108 86 libtorch.cc:1917] TRITONBACKEND_Initialize: pytorch\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.920145 86 libtorch.cc:1927] Triton TRITONBACKEND API version: 1.10\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.920151 86 libtorch.cc:1933] 'pytorch' TRITONBACKEND API version: 1.10\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.920170 86 libtorch.cc:1966] TRITONBACKEND_ModelInitialize: ncf_food_model (version 1)\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.920957 86 model_config_utils.cc:1656] ModelConfig 64-bit fields:\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.920977 86 model_config_utils.cc:1658] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.920981 86 model_config_utils.cc:1658] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.920999 86 model_config_utils.cc:1658] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921003 86 model_config_utils.cc:1658] \tModelConfig::ensemble_scheduling::step::model_version\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921008 86 model_config_utils.cc:1658] \tModelConfig::input::dims\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921013 86 model_config_utils.cc:1658] \tModelConfig::input::reshape::shape\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921017 86 model_config_utils.cc:1658] \tModelConfig::instance_group::secondary_devices::device_id\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921022 86 model_config_utils.cc:1658] \tModelConfig::model_warmup::inputs::value::dims\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921026 86 model_config_utils.cc:1658] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921040 86 model_config_utils.cc:1658] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921044 86 model_config_utils.cc:1658] \tModelConfig::output::dims\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921049 86 model_config_utils.cc:1658] \tModelConfig::output::reshape::shape\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921052 86 model_config_utils.cc:1658] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921074 86 model_config_utils.cc:1658] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921079 86 model_config_utils.cc:1658] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921086 86 model_config_utils.cc:1658] \tModelConfig::sequence_batching::state::dims\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921090 86 model_config_utils.cc:1658] \tModelConfig::sequence_batching::state::initial_state::dims\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.921095 86 model_config_utils.cc:1658] \tModelConfig::version_policy::specific::versions\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m W0628 03:05:06.921196 86 libtorch.cc:262] skipping model configuration auto-complete for 'ncf_food_model': not supported for pytorch backend\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.922595 86 libtorch.cc:291] Optimized execution is enabled for model instance 'ncf_food_model'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.922614 86 libtorch.cc:310] Cache Cleaning is disabled for model instance 'ncf_food_model'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.922619 86 libtorch.cc:327] Inference Mode is disabled for model instance 'ncf_food_model'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.922635 86 libtorch.cc:422] NvFuser is not specified for model instance 'ncf_food_model'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.923631 86 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: ncf_food_model (GPU device 0)\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:06.925073 86 backend_model_instance.cc:105] Creating instance ncf_food_model on GPU 0 (8.6) using artifact 'model.pt'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:07.688447 86 backend_model_instance.cc:735] Starting backend thread for ncf_food_model at nice 0 on device 0...\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:07.688588 86 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: ncf_food_model (GPU device 1)\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:07.688891 86 backend_model_instance.cc:105] Creating instance ncf_food_model on GPU 1 (8.6) using artifact 'model.pt'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:08.311431 86 backend_model_instance.cc:735] Starting backend thread for ncf_food_model at nice 0 on device 1...\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:08.311548 86 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: ncf_food_model (GPU device 2)\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:08.311849 86 backend_model_instance.cc:105] Creating instance ncf_food_model on GPU 2 (8.6) using artifact 'model.pt'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:08.936774 86 backend_model_instance.cc:735] Starting backend thread for ncf_food_model at nice 0 on device 2...\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:08.936906 86 libtorch.cc:2010] TRITONBACKEND_ModelInstanceInitialize: ncf_food_model (GPU device 3)\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:08.937207 86 backend_model_instance.cc:105] Creating instance ncf_food_model on GPU 3 (8.6) using artifact 'model.pt'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.557450 86 backend_model_instance.cc:735] Starting backend thread for ncf_food_model at nice 0 on device 3...\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.557606 86 model_repository_manager.cc:1352] successfully loaded 'ncf_food_model' version 1\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.557639 86 model_repository_manager.cc:1151] TriggerNextAction() 'ncf_food_model' version 1: 0\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.557647 86 model_repository_manager.cc:1165] no next action, trigger OnComplete()\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.557725 86 model_repository_manager.cc:728] VersionStates() 'ncf_food_model'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.557782 86 model_repository_manager.cc:728] VersionStates() 'ncf_food_model'\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.557854 86 server.cc:559] \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +------------------+------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | Repository Agent | Path |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +------------------+------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +------------------+------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.557939 86 server.cc:586] \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +---------+---------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | Backend | Path                                                    | Config                                                                                                                                                        |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +---------+---------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | pytorch | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so | {\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +---------+---------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.557975 86 model_repository_manager.cc:704] ModelStates()\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.558036 86 server.cc:629] \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +----------------+---------+--------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | Model          | Version | Status |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +----------------+---------+--------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | ncf_food_model | 1       | READY  |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +----------------+---------+--------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.558262 86 tritonserver.cc:2176] \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | Option                           | Value                                                                                                                                                                                        |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | server_id                        | triton                                                                                                                                                                                       |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | server_version                   | 2.24.0                                                                                                                                                                                       |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | model_repository_path[0]         | /opt/ml/model/                                                                                                                                                                               |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | model_control_mode               | MODE_EXPLICIT                                                                                                                                                                                |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | startup_models_0                 | ncf_food_model                                                                                                                                                                               |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | strict_model_config              | 0                                                                                                                                                                                            |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | rate_limit                       | OFF                                                                                                                                                                                          |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | cuda_memory_pool_byte_size{1}    | 67108864                                                                                                                                                                                     |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | cuda_memory_pool_byte_size{2}    | 67108864                                                                                                                                                                                     |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | cuda_memory_pool_byte_size{3}    | 67108864                                                                                                                                                                                     |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | response_cache_byte_size         | 0                                                                                                                                                                                            |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | min_supported_compute_capability | 6.0                                                                                                                                                                                          |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | strict_readiness                 | 1                                                                                                                                                                                            |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m | exit_timeout                     | 30                                                                                                                                                                                           |\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:09.558649 86 sagemaker_server.cc:280] Started Sagemaker HTTPService at 0.0.0.0:8080\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:10.151670 86 sagemaker_server.cc:190] SageMaker request: 0 /ping\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:10.151707 86 model_repository_manager.cc:704] ModelStates()\n",
      "!"
     ]
    }
   ],
   "source": [
    "localPredictor = localPytorchModel.deploy(\n",
    "    instance_type=strInstanceType,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=strEndpointName,\n",
    "    wait=True,\n",
    "    log=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload:  {'inputs': [{'name': 'INPUT__0', 'shape': [1, 100], 'datatype': 'INT32', 'data': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}, {'name': 'INPUT__1', 'shape': [1, 100], 'datatype': 'INT32', 'data': [[568, 755, 898, 155, 244, 6, 25, 283, 256, 685, 65, 893, 19, 286, 149, 85, 556, 984, 264, 386, 556, 543, 714, 510, 160, 107, 721, 842, 332, 868, 472, 994, 146, 475, 995, 706, 891, 918, 555, 334, 942, 995, 556, 271, 715, 183, 84, 572, 914, 288, 871, 287, 465, 477, 158, 815, 201, 789, 777, 78, 839, 65, 118, 670, 342, 341, 615, 528, 729, 386, 445, 138, 793, 352, 349, 187, 23, 9, 88, 834, 826, 644, 230, 885, 834, 318, 536, 462, 792, 203, 995, 978, 51, 895, 748, 400, 212, 332, 962, 491]]}]}\n"
     ]
    }
   ],
   "source": [
    "def create_sample_payload():\n",
    "    # user\n",
    "    user_np = np.zeros((1,100)).astype(np.int32)\n",
    "    # item\n",
    "    item_np = np.random.randint(low=1, high=1000, size=(1,100)).astype(np.int32)\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\"name\": \"INPUT__0\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": user_np.tolist()},\n",
    "            {\"name\": \"INPUT__1\", \"shape\": [1,100], \n",
    "             \"datatype\": \"INT32\", \"data\": item_np.tolist()},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return payload\n",
    "\n",
    "payload = create_sample_payload()\n",
    "print(\"payload: \", payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.224234 86 sagemaker_server.cc:190] SageMaker request: 2 /invocations\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.224306 86 model_repository_manager.cc:773] GetModel() 'ncf_food_model' version -1\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.224326 86 model_repository_manager.cc:773] GetModel() 'ncf_food_model' version -1\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.224402 86 infer_request.cc:713] [request id: <id_unknown>] prepared: [0x0x7f2df4004c80] request id: , model: ncf_food_model, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 1, priority: 0, timeout (us): 0\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m original inputs:\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m [0x0x7f2df4015678] input: INPUT__1, type: INT32, original shape: [1,100], batch + shape: [1,100], shape: [100]\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m [0x0x7f2df4015258] input: INPUT__0, type: INT32, original shape: [1,100], batch + shape: [1,100], shape: [100]\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m override inputs:\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m inputs:\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m [0x0x7f2df4015258] input: INPUT__0, type: INT32, original shape: [1,100], batch + shape: [1,100], shape: [100]\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m [0x0x7f2df4015678] input: INPUT__1, type: INT32, original shape: [1,100], batch + shape: [1,100], shape: [100]\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m original requested outputs:\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m requested outputs:\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m OUTPUT__0\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m \n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.224488 86 libtorch.cc:2076] model ncf_food_model, instance ncf_food_model, executing 1 requests\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.224512 86 libtorch.cc:947] TRITONBACKEND_ModelExecute: Running ncf_food_model with 1 requests\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.224562 86 pinned_memory_manager.cc:161] pinned memory allocation: size 400, addr 0x7f3018000090\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.224642 86 pinned_memory_manager.cc:161] pinned memory allocation: size 400, addr 0x7f3018000230\n",
      "result : \u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.947651 86 infer_response.cc:167] add response output: output: OUTPUT__0, type: FP32, shape: [1,100,1]\n",
      " {'model_name': 'ncf_food_model', 'model_version': '1', 'outputs': [{'name': 'OUTPUT__0', 'datatype': 'FP32', 'shape': [1, 100, 1], 'data': [1.513258457183838, -4.164857387542725, -1.331531047821045, -2.194366931915283, -1.9439448118209839, 1.5431718826293945, 1.042938232421875, 1.82234525680542, -5.435118198394775, -1.092459797859192, -0.5559927225112915, -3.8147189617156982, 0.684249758720398, -3.852769374847412, -2.1459996700286865, -1.5711619853973389, -1.7939796447753906, -4.281248569488525, -1.2939186096191406, -0.37167778611183167, 1.660508632659912, -3.4373342990875244, -1.1017006635665894, -0.7232743501663208, -0.16845020651817322, -1.437998652458191, 0.7327100038528442, -0.9741780757904053, -0.3523409068584442, 1.3448190689086914, -2.3961732387542725, -1.1790937185287476, -3.852769374847412, -2.367541790008545, -1.112399935722351, -1.498124599456787, -2.5152997970581055, -0.6000606417655945, 1.4025909900665283, -4.144300937652588, -3.3157570362091064, 3.1749632358551025, -3.678889274597168, -1.6394495964050293, 0.7442549467086792, 0.3664093315601349, 0.6210218667984009, 2.1828927993774414, -1.574058175086975, 2.395418643951416, -1.8964701890945435, -1.213133692741394, -1.5652754306793213, -0.9879721403121948, 0.8540316820144653, -2.2890100479125977, -0.5242491364479065, -0.7500121593475342, -1.0445852279663086, -1.8111308813095093, -2.8349695205688477, -0.37167778611183167, -0.400210976600647, -2.972346544265747, 0.21269407868385315, -4.440776824951172, -6.229286193847656, -1.6427546739578247, -3.5684309005737305, 1.5040534734725952, -0.7857266664505005, -2.3961732387542725, 1.0160274505615234, -4.75848388671875, 0.2658598721027374, -1.3878406286239624, -1.3884978294372559, 1.6680989265441895, -3.426793098449707, -2.3833463191986084, -0.18121762573719025, -0.761283278465271, -0.7500121593475342, -4.453303337097168, -1.6810879707336426, 0.986011266708374, -0.7064753770828247, -1.942049503326416, -0.635543942451477, 2.224625587463379, -2.482563018798828, -0.35986635088920593, -0.7417725324630737, -2.642673969268799, -0.7379575967788696, -3.6410810947418213, 2.395418643951416, -2.9482412338256836, 0.960182785987854, 2.0351483821868896]}]}\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.947698 86 http_server.cc:1088] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.947705 86 http_server.cc:1108] HTTP using buffer for: 'OUTPUT__0', size: 400, addr: 0x7f2e5c0dd1c0\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.947728 86 pinned_memory_manager.cc:161] pinned memory allocation: size 400, addr 0x7f30180003d0\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.947761 86 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7f30180003d0\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.947837 86 http_server.cc:1182] HTTP release: size 400, addr 0x7f2e5c0dd1c0\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.947870 86 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7f3018000090\n",
      "\u001b[36musbfufy3dj-algo-1-stjkc |\u001b[0m I0628 03:05:40.947884 86 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7f3018000230\n"
     ]
    }
   ],
   "source": [
    "def single_model_invoke_endpoint(client,endpoint_name, payload): \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/octet-stream\", \n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "\n",
    "    result = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "    \n",
    "    return result\n",
    "\n",
    "runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()    \n",
    "result = single_model_invoke_endpoint(runtime_client,strEndpointName, payload)\n",
    "print(\"result : \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.inference_utils import delete_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "--- Deleted model: js-onboarding-2023-06-28-03-05-04-782\n",
      "--- Deleted endpoint_config: triton-ncf-ep-2023-06-28-03-05-03\n",
      "--- Deleted endpoint: triton-ncf-ep-2023-06-28-03-05-03\n"
     ]
    }
   ],
   "source": [
    "client = sagemaker.local.LocalSagemakerClient()\n",
    "delete_endpoint(client, strEndpointName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cloud mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Depoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicContainer = {\n",
    "    \"Image\": strEcrRepositoryUri,\n",
    "    \"ModelDataUrl\": strModelUriPt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dicContainer: {'Image': '419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest', 'ModelDataUrl': 's3://sagemaker-us-east-1-419974056037/triton-ncf/ncf_food_model.model.tar.gz'}\n",
      "strSMModelName: triton-ncf-mdl-2023-06-28-03-08-35\n"
     ]
    }
   ],
   "source": [
    "print(f'dicContainer: {dicContainer}')\n",
    "print(f'strSMModelName: {strSMModelName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=strSMModelName,\n",
    "    ExecutionRoleArn=strExecutionRole,\n",
    "    PrimaryContainer=dicContainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-1:419974056037:model/triton-ncf-mdl-2023-06-28-03-08-35\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Arn: {create_model_response[\"ModelArn\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=strEndpointConfigName,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": strInstanceType,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": strSMModelName,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:419974056037:endpoint-config/triton-ncf-epc-2023-06-28-03-08-35\n"
     ]
    }
   ],
   "source": [
    "print(f'Endpoint Config Arn: {create_endpoint_config_response[\"EndpointConfigArn\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=strEndpointName,\n",
    "    EndpointConfigName=strEndpointConfigName\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:419974056037:endpoint/triton-ncf-ep-2023-06-28-03-08-35\n"
     ]
    }
   ],
   "source": [
    "print(f'Endpoint Arn: {create_endpoint_response[\"EndpointArn\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=strEndpointName)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=strEndpointName)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'ncf_food_model',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'OUTPUT__0',\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [1, 100, 1],\n",
       "   'data': [-2.661646604537964,\n",
       "    -0.9603654146194458,\n",
       "    0.341450959444046,\n",
       "    0.16189703345298767,\n",
       "    -0.7379575967788696,\n",
       "    -3.8147194385528564,\n",
       "    0.43504074215888977,\n",
       "    -2.7857987880706787,\n",
       "    -0.7051801681518555,\n",
       "    -1.9214153289794922,\n",
       "    -2.4508779048919678,\n",
       "    -1.4815970659255981,\n",
       "    -0.840631365776062,\n",
       "    -0.3676549792289734,\n",
       "    -2.83782958984375,\n",
       "    -4.166333198547363,\n",
       "    0.8842571973800659,\n",
       "    -2.8424930572509766,\n",
       "    -0.37986236810684204,\n",
       "    -1.6701970100402832,\n",
       "    -2.5594990253448486,\n",
       "    -0.49171993136405945,\n",
       "    -0.19276882708072662,\n",
       "    -2.5152997970581055,\n",
       "    -3.843963146209717,\n",
       "    -1.0526206493377686,\n",
       "    0.024429410696029663,\n",
       "    -0.3725481629371643,\n",
       "    2.967946767807007,\n",
       "    0.37049928307533264,\n",
       "    -2.642673969268799,\n",
       "    -2.519130229949951,\n",
       "    -2.6336967945098877,\n",
       "    -3.574371337890625,\n",
       "    -3.840346574783325,\n",
       "    1.5517158508300781,\n",
       "    -2.93843412399292,\n",
       "    -3.856576442718506,\n",
       "    -3.2103230953216553,\n",
       "    -1.125656008720398,\n",
       "    1.7070531845092773,\n",
       "    -4.472683906555176,\n",
       "    -2.361156940460205,\n",
       "    -1.9214153289794922,\n",
       "    0.5596611499786377,\n",
       "    -3.7477192878723145,\n",
       "    -1.0494422912597656,\n",
       "    -4.286262512207031,\n",
       "    2.2152211666107178,\n",
       "    -3.5722107887268066,\n",
       "    2.2152211666107178,\n",
       "    -1.9471379518508911,\n",
       "    -4.523408889770508,\n",
       "    -1.3915296792984009,\n",
       "    -2.3864002227783203,\n",
       "    0.863257646560669,\n",
       "    -2.367541790008545,\n",
       "    1.8418753147125244,\n",
       "    -2.180792808532715,\n",
       "    -4.150918006896973,\n",
       "    -2.8590402603149414,\n",
       "    -1.911362648010254,\n",
       "    1.1412601470947266,\n",
       "    0.35145047307014465,\n",
       "    -2.5407629013061523,\n",
       "    -1.2422709465026855,\n",
       "    -1.3279616832733154,\n",
       "    -2.4032278060913086,\n",
       "    -0.18121759593486786,\n",
       "    -4.725953578948975,\n",
       "    2.0351483821868896,\n",
       "    -3.6643736362457275,\n",
       "    -0.9799041748046875,\n",
       "    -1.91647207736969,\n",
       "    1.4186033010482788,\n",
       "    -2.114771604537964,\n",
       "    -1.9553580284118652,\n",
       "    -2.03305721282959,\n",
       "    -1.74408757686615,\n",
       "    -0.7232744693756104,\n",
       "    -2.3597400188446045,\n",
       "    -2.0837340354919434,\n",
       "    -0.1162126362323761,\n",
       "    -2.2602033615112305,\n",
       "    -4.615566730499268,\n",
       "    -4.920835971832275,\n",
       "    -0.0005023777484893799,\n",
       "    -4.696341037750244,\n",
       "    -0.6355441212654114,\n",
       "    1.0508571863174438,\n",
       "    -0.7861685752868652,\n",
       "    0.028661489486694336,\n",
       "    -4.115433692932129,\n",
       "    -3.4083609580993652,\n",
       "    0.42010655999183655,\n",
       "    -2.799952983856201,\n",
       "    -1.8415679931640625,\n",
       "    -4.070901870727539,\n",
       "    -0.6836743354797363,\n",
       "    -1.0987564325332642]}]}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "single_model_invoke_endpoint(runtime_client,strEndpointName, payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Deleted model: triton-ncf-mdl-2023-06-28-03-08-35\n",
      "--- Deleted endpoint_config: triton-ncf-epc-2023-06-28-03-08-35\n",
      "--- Deleted endpoint: triton-ncf-ep-2023-06-28-03-08-35\n"
     ]
    }
   ],
   "source": [
    "client = boto3.Session().client('sagemaker')\n",
    "delete_endpoint(client, strEndpointName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create an ML Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role, ModelPackage\n",
    "import time\n",
    "\n",
    "# Common variables\n",
    "session = sage.Session()\n",
    "s3_bucket = session.default_bucket()\n",
    "region = session.boto_region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_client = session.boto_session.client(\"s3\")\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see how you can package your artifacts (ECR image and the trained model artifacts) into a ModelPackage. Once you complete this, you can list your product as a pretrained model in the AWS Marketplace.\n",
    "\n",
    "**NOTE:** If your model can be deployed on multiple hardware types (CPU/GPU/Inferentia) then a ModelPackage must be created for each and added to the MP listing as different versions as, in general, the container image used will be different for each.  \n",
    "\n",
    "#### Model Package Definition\n",
    "A Model Package is a reusable abstraction for model artifacts that packages all the ingredients necessary for inference. It consists of an inference specification that defines the inference image to use along with an optional model data location.\n",
    "\n",
    "The ModelPackage must be created in the AWS account that will be registered as a seller on the AWS Marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "model_name = \"marketplace-model-test\"#\"<<YourModelName>>\"\n",
    "model_description = \"marketplace-model-test\"#\"<<YourModelDescription>>\"\n",
    "\n",
    "# <<YourSupportedContentTypes>>\n",
    "supported_content_types = [\"application/octet-stream\"]#[\"text/csv\", \"application/json\", \"application/jsonlines\"]\n",
    "\n",
    "# <<YourSupportedResponseMIMETypes>>\n",
    "supported_response_MIME_types = [ \n",
    "    \"application/json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Model Package creation process requires you to specify following:\n",
    "  1. Docker image\n",
    "  2. Model artifacts\n",
    "    - You can either package these inside the docker image, as we have done in this example, or provide them as a gzipped tarball.\n",
    "    - In the case of large Models gzipped tarball is required. \n",
    "  3. Validation specification \n",
    "        \n",
    "In order to provide confidence to sellers (and buyers) that the products work in Amazon SageMaker, before listing them on AWS Marketplace SageMaker needs to perform basic validations. The product can be listed in AWS Marketplace only if this validation process succeeds. This validation process uses the validation profile and sample data provided by you to create a transform job in your account using the Model to verify your inference image works with SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to identify the right instance-sizes for your ML models. You can do so by running performance tests on top of your ML Model.\n",
    "\n",
    "**NOTE:** In addition to tuning, take into account the requirements of your model when identifying instance types.  If your model does not use GPU resources, then do not include GPU instance types.  Similarly, if your model does use GPU resources, but can only make use of a single GPU, do not include instance types that have multiple GPUs as it will lead to increased infrastructure charges for your customers with no performance benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "supported_realtime_inference_instance_types = [\"ml.g5.12xlarge\"]#[\"<<YourModelSupportInstanceType>>\"]\n",
    "supported_batch_transform_instance_types = [\"ml.g4dn.12xlarge\"] #  use either a g4dn.12xlarge or p3.8xlarge. However, the Batch Transform validation step is not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_file_name = \"input.jsonl\"\n",
    "validation_input_path = f\"s3://{s3_bucket}/validation-input-json/\"\n",
    "validation_output_path = f\"s3://{s3_bucket}/validation-output-jsonl/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Not required)* First, we create sample data to be used in the validation stage of the ModelPackage creation and upload it to S3. This sample data would need to be in the format your model expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'N1C5PHHZKP8DAZBP',\n",
       "  'HostId': 'CuwpNHIU7kpQ4DYMzfeq2FU97EtCNOJlCa0KgJpSrEa8DLLD3Gp2rzWFUZQXB5oKwCfhegdtUIRQQzkBRmHW8Q==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'CuwpNHIU7kpQ4DYMzfeq2FU97EtCNOJlCa0KgJpSrEa8DLLD3Gp2rzWFUZQXB5oKwCfhegdtUIRQQzkBRmHW8Q==',\n",
       "   'x-amz-request-id': 'N1C5PHHZKP8DAZBP',\n",
       "   'date': 'Wed, 28 Jun 2023 04:07:13 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"1ac97b608a183346185ba9a2f1794283\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"1ac97b608a183346185ba9a2f1794283\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_line = json.dumps(payload)\n",
    "with open(\"input.jsonl\", \"w\") as f:\n",
    "    f.write(json_line)\n",
    "s3_client.put_object(Bucket=s3_bucket, Key=\"validation-input-json/input.jsonl\", Body=json_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print (strEcrRepositoryUri)\n",
    "# print (strModelUriPt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Model Package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docker_image_uri = \"419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest\"\n",
    "model_data_location = \"s3://sagemaker-us-east-1-419974056037/triton-ncf/ncf_food_model.model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# docker_image_uri = strEcrRepositoryUri#\"<<YourModelImageURI>>\" # ECR URI of Image used to host model\n",
    "# model_data_location = strModelUriPt#\"<<YourModelS3Location>>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating the ModelPackage you will recieve the error:\n",
    "\n",
    "```\n",
    "~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/validate.py in serialize_to_request(self, parameters, operation_model)\n",
    "    380             if report.has_errors():\n",
    "--> 381                raise ParamValidationError(report=report.generate_report())\n",
    "    382         return self._serializer.serialize_to_request(\n",
    "    383             parameters, operation_model\n",
    "\n",
    "ParamValidationError: Parameter validation failed:\n",
    "Invalid length for parameter ValidationSpecification.ValidationProfiles, value: 0, valid min length: 1\n",
    "```\n",
    "\n",
    "In order to resolve this issue, open the `validate.py` file in this case it is located at `~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/validate.py`. Remove/comment out the following code: \n",
    "\n",
    "```\n",
    "380 if report.has_errors():\n",
    "381                 raise ParamValidationError(report=report.generate_report())\n",
    "```\n",
    "\n",
    "Restart the Kernal, import boto3 again and re-run the cell. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!which python** <BR>\n",
    "**for pytorch_p310: cd ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/validate.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package = session.sagemaker_client.create_model_package(\n",
    "    ModelPackageName=model_name,\n",
    "    ModelPackageDescription=model_description,\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": docker_image_uri,\n",
    "                \"ModelDataUrl\": model_data_location\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedTransformInstanceTypes\": supported_batch_transform_instance_types,\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": supported_realtime_inference_instance_types,\n",
    "        \"SupportedContentTypes\": supported_content_types,\n",
    "        \"SupportedResponseMIMETypes\": supported_response_MIME_types,\n",
    "    },\n",
    "    CertifyForMarketplace=True,  # Make sure to set this to True\n",
    "    #ValidationSpecification={\n",
    "    #    \"ValidationRole\": role,\n",
    "    #    \"ValidationProfiles\": [],\n",
    "    #},\n",
    "    ValidationSpecification={\n",
    "        'ValidationRole': role,\n",
    "        'ValidationProfiles': [\n",
    "            {\n",
    "                'ProfileName': \"validation\",\n",
    "                'TransformJobDefinition': {\n",
    "                    'MaxConcurrentTransforms': 1,\n",
    "                    'MaxPayloadInMB': 64,\n",
    "                    'BatchStrategy': 'SingleRecord',\n",
    "                    #'Environment': {\n",
    "                    #    'string': 'string'\n",
    "                    #},\n",
    "                    'TransformInput': {\n",
    "                        'DataSource': {\n",
    "                            'S3DataSource': {\n",
    "                                'S3DataType': 'S3Prefix',\n",
    "                                'S3Uri': \"s3://sagemaker-us-east-1-419974056037/validation-input-json/input.jsonl\"\n",
    "                            }\n",
    "                        },\n",
    "                        'ContentType': 'application/octet-stream',\n",
    "                        'CompressionType': 'None',\n",
    "                        'SplitType': 'None'\n",
    "                    },\n",
    "                    'TransformOutput': {\n",
    "                        'S3OutputPath': 's3://sagemaker-us-east-1-419974056037/validation-output-json/output.json',\n",
    "                        'Accept': 'application/json',\n",
    "                        'AssembleWith': 'None',\n",
    "                        #'KmsKeyId': 'string'\n",
    "                    },\n",
    "                    'TransformResources': {\n",
    "                        'InstanceType': 'ml.g4dn.12xlarge',\n",
    "                        'InstanceCount': 1,\n",
    "                        #'VolumeKmsKeyId': 'string'\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ModelPackageName': 'marketplace-model-test',\n",
       " 'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:419974056037:model-package/marketplace-model-test',\n",
       " 'ModelPackageDescription': 'marketplace-model-test',\n",
       " 'CreationTime': datetime.datetime(2023, 6, 28, 4, 37, 6, 954000, tzinfo=tzlocal()),\n",
       " 'InferenceSpecification': {'Containers': [{'Image': '419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding:latest',\n",
       "    'ImageDigest': 'sha256:d129f5ed05ae9517a76176061bf0987eef0ea5a83002a142e100990c9acd392a',\n",
       "    'ModelDataUrl': 's3://sagemaker-us-east-1-419974056037/triton-ncf/ncf_food_model.model.tar.gz'}],\n",
       "  'SupportedTransformInstanceTypes': ['ml.g4dn.12xlarge'],\n",
       "  'SupportedRealtimeInferenceInstanceTypes': ['ml.g5.12xlarge'],\n",
       "  'SupportedContentTypes': ['application/octet-stream'],\n",
       "  'SupportedResponseMIMETypes': ['application/json']},\n",
       " 'ValidationSpecification': {'ValidationRole': 'arn:aws:iam::419974056037:role/service-role/AmazonSageMaker-ExecutionRole-20221206T163436',\n",
       "  'ValidationProfiles': [{'ProfileName': 'validation',\n",
       "    'TransformJobDefinition': {'MaxConcurrentTransforms': 1,\n",
       "     'MaxPayloadInMB': 64,\n",
       "     'BatchStrategy': 'SingleRecord',\n",
       "     'TransformInput': {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': 's3://sagemaker-us-east-1-419974056037/validation-input-json/input.jsonl'}},\n",
       "      'ContentType': 'application/octet-stream',\n",
       "      'CompressionType': 'None',\n",
       "      'SplitType': 'None'},\n",
       "     'TransformOutput': {'S3OutputPath': 's3://sagemaker-us-east-1-419974056037/validation-output-json/output.json',\n",
       "      'Accept': 'application/json',\n",
       "      'AssembleWith': 'None',\n",
       "      'KmsKeyId': ''},\n",
       "     'TransformResources': {'InstanceType': 'ml.g4dn.12xlarge',\n",
       "      'InstanceCount': 1}}}]},\n",
       " 'ModelPackageStatus': 'Completed',\n",
       " 'ModelPackageStatusDetails': {'ValidationStatuses': [{'Name': 'validation',\n",
       "    'Status': 'Completed'}],\n",
       "  'ImageScanStatuses': [{'Name': '419974056037.dkr.ecr.us-east-1.amazonaws.com/js-onboarding@sha256:d129f5ed05ae9517a76176061bf0987eef0ea5a83002a142e100990c9acd392a',\n",
       "    'Status': 'Completed'}]},\n",
       " 'CertifyForMarketplace': True,\n",
       " 'CreatedBy': {'IamIdentity': {'Arn': 'arn:aws:sts::419974056037:assumed-role/AmazonSageMaker-ExecutionRole-20221206T163436/SageMaker',\n",
       "   'PrincipalId': 'AROAWDSDHRBSUJHH634B5:SageMaker'}},\n",
       " 'ResponseMetadata': {'RequestId': 'ecf6fa09-8909-48ef-8569-70988e48e992',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ecf6fa09-8909-48ef-8569-70988e48e992',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2057',\n",
       "   'date': 'Wed, 28 Jun 2023 04:43:49 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.wait_for_model_package(model_package_name=model_name) # If failure occurs navigate to SageMaker Console > My marketplace model packages > select the failed ModelPackage for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the preceding cell, open the [Model Packages console from Amazon SageMaker](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources) and check if model creation succeeded. \n",
    "\n",
    "Choose the Model and then open the **Validation** tab to see the validation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate model in Amazon SageMaker environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a deployable model from the model package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package[\"ModelPackageArn\"],\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Validate Real-time inference via Amazon SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy the SageMaker model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'marketplace-model-test'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=supported_realtime_inference_instance_types[0],\n",
    "    endpoint_name=model_name,\n",
    ")\n",
    "model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type = supported_content_types[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example invocation via boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'ncf_food_model',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'OUTPUT__0',\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [1, 100, 1],\n",
       "   'data': [-1.796771764755249,\n",
       "    -1.6427546739578247,\n",
       "    -2.4718635082244873,\n",
       "    -1.281156063079834,\n",
       "    2.3520302772521973,\n",
       "    -0.7353585958480835,\n",
       "    1.2487637996673584,\n",
       "    1.7070531845092773,\n",
       "    0.8721898794174194,\n",
       "    -0.7815717458724976,\n",
       "    -4.7818145751953125,\n",
       "    -2.924468994140625,\n",
       "    0.6013489961624146,\n",
       "    -2.3256564140319824,\n",
       "    1.301037311553955,\n",
       "    1.1565217971801758,\n",
       "    -0.6119728088378906,\n",
       "    -2.7658627033233643,\n",
       "    -0.13805341720581055,\n",
       "    -1.6411755084991455,\n",
       "    -0.6119728088378906,\n",
       "    -3.5467724800109863,\n",
       "    -0.9603652954101562,\n",
       "    0.8064634799957275,\n",
       "    -0.5977281332015991,\n",
       "    -3.7661967277526855,\n",
       "    -0.7232743501663208,\n",
       "    -1.699474573135376,\n",
       "    -0.7003310322761536,\n",
       "    -1.4859449863433838,\n",
       "    -0.3064301908016205,\n",
       "    0.2435605823993683,\n",
       "    -3.6060190200805664,\n",
       "    -0.2855709195137024,\n",
       "    -4.563620090484619,\n",
       "    -0.9588063955307007,\n",
       "    -1.5518157482147217,\n",
       "    -1.6185109615325928,\n",
       "    -3.140951633453369,\n",
       "    -1.8111308813095093,\n",
       "    -2.6135094165802,\n",
       "    -4.563620090484619,\n",
       "    -0.6119728088378906,\n",
       "    -2.1501665115356445,\n",
       "    -3.188199996948242,\n",
       "    -0.09216740727424622,\n",
       "    1.13021981716156,\n",
       "    -2.4032278060913086,\n",
       "    -3.306879997253418,\n",
       "    -0.6818251609802246,\n",
       "    -0.4533068835735321,\n",
       "    -1.1406084299087524,\n",
       "    -1.0526206493377686,\n",
       "    -1.9578197002410889,\n",
       "    -1.3431861400604248,\n",
       "    -0.761283278465271,\n",
       "    -2.028411865234375,\n",
       "    -3.751523494720459,\n",
       "    -4.381546974182129,\n",
       "    -0.1604379266500473,\n",
       "    -1.2868938446044922,\n",
       "    -4.7818145751953125,\n",
       "    -2.625575065612793,\n",
       "    0.18595513701438904,\n",
       "    0.26044026017189026,\n",
       "    -3.168252944946289,\n",
       "    -3.6643733978271484,\n",
       "    -0.9190118312835693,\n",
       "    1.0237303972244263,\n",
       "    -1.6411755084991455,\n",
       "    -2.330018997192383,\n",
       "    -0.5242491364479065,\n",
       "    -1.3949731588363647,\n",
       "    -0.09413963556289673,\n",
       "    0.15071287751197815,\n",
       "    -1.5635590553283691,\n",
       "    2.3875041007995605,\n",
       "    2.224625587463379,\n",
       "    0.855963945388794,\n",
       "    -2.7165842056274414,\n",
       "    -1.9287049770355225,\n",
       "    -2.706181526184082,\n",
       "    -2.6396236419677734,\n",
       "    -1.2365562915802002,\n",
       "    -2.7165842056274414,\n",
       "    -3.895315170288086,\n",
       "    -2.8424930572509766,\n",
       "    1.1959030628204346,\n",
       "    -1.498124599456787,\n",
       "    -0.1162126362323761,\n",
       "    -4.563620090484619,\n",
       "    -1.8849313259124756,\n",
       "    2.3503122329711914,\n",
       "    -0.9040132761001587,\n",
       "    -1.310408115386963,\n",
       "    0.19643107056617737,\n",
       "    -2.2614352703094482,\n",
       "    -0.7003310322761536,\n",
       "    -5.547597885131836,\n",
       "    0.8340789079666138]}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make use of your own example input data to test the Endpoint\n",
    "#input_json = '{\"text\": \"sample\"}'\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=model.endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "json.load(response[\"Body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example invocation via the AWS CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ContentType\": \"application/json\",\n",
      "    \"InvokedProductionVariant\": \"AllTraffic\"\n",
      "}\n",
      "{\"model_name\":\"ncf_food_model\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"OUTPUT__0\",\"datatype\":\"FP32\",\"shape\":[1,100,1],\"data\":[-1.796771764755249,-1.6427546739578248,-2.4718635082244875,-1.281156063079834,2.3520302772521974,-0.7353585958480835,1.2487637996673585,1.7070531845092774,0.8721898794174194,-0.7815717458724976,-4.7818145751953129,-2.924468994140625,0.6013489961624146,-2.3256564140319826,1.301037311553955,1.1565217971801758,-0.6119728088378906,-2.7658627033233644,-0.13805341720581056,-1.6411755084991456,-0.6119728088378906,-3.5467724800109865,-0.9603652954101563,0.8064634799957275,-0.5977281332015991,-3.7661967277526857,-0.7232743501663208,-1.699474573135376,-0.7003310322761536,-1.4859449863433838,-0.3064301908016205,0.24356058239936829,-3.6060190200805666,-0.2855709195137024,-4.563620090484619,-0.9588063955307007,-1.5518157482147217,-1.6185109615325928,-3.140951633453369,-1.8111308813095093,-2.6135094165802,-4.563620090484619,-0.6119728088378906,-2.1501665115356447,-3.188199996948242,-0.09216740727424622,1.13021981716156,-2.4032278060913088,-3.306879997253418,-0.6818251609802246,-0.4533068835735321,-1.1406084299087525,-1.0526206493377686,-1.9578197002410889,-1.3431861400604249,-0.761283278465271,-2.028411865234375,-3.751523494720459,-4.381546974182129,-0.1604379266500473,-1.2868938446044922,-4.7818145751953129,-2.625575065612793,0.18595513701438905,0.26044026017189028,-3.168252944946289,-3.6643733978271486,-0.9190118312835693,1.0237303972244263,-1.6411755084991456,-2.330018997192383,-0.5242491364479065,-1.3949731588363648,-0.09413963556289673,0.15071287751197816,-1.5635590553283692,2.3875041007995607,2.224625587463379,0.855963945388794,-2.7165842056274416,-1.9287049770355225,-2.706181526184082,-2.6396236419677736,-1.2365562915802003,-2.7165842056274416,-3.895315170288086,-2.8424930572509767,1.1959030628204346,-1.498124599456787,-0.1162126362323761,-4.563620090484619,-1.8849313259124756,2.3503122329711916,-0.9040132761001587,-1.310408115386963,0.19643107056617738,-2.2614352703094484,-0.7003310322761536,-5.547597885131836,0.8340789079666138]}]}"
     ]
    }
   ],
   "source": [
    "# Perform inference\n",
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "    --endpoint-name $model.endpoint_name \\\n",
    "    --body fileb://$validation_file_name \\\n",
    "    --content-type $content_type \\\n",
    "    --region $session.boto_region_name \\\n",
    "    out.out\n",
    "    \n",
    "    \n",
    "# Print inference\n",
    "!head out.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the endpoint and endpoint configuration created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model.endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Since the model is not required, you can delete it. Note that you are deleting the deployable model. Not the model package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To publish the model to the AWS Marketplace, you will need to specify model package ARN. Copy the following Model Package ARN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:us-east-1:419974056037:model-package/marketplace-model-test'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_package[\"ModelPackageArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"step6\"></a>Step 6: List ML Model on AWS Marketplace\n",
    "\n",
    "\n",
    "1. Model Partner creates [public profile](https://docs.aws.amazon.com/marketplace/latest/userguide/seller-registration-process.html#seller-public-profile) on AWS Marketplace and registers to be a seller.\n",
    "There is no need to provide Tax information as the product on Marketplace will be listed as free.\n",
    "\n",
    "2. In the [Model Packages](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/model-packages/my-resources) section of the SageMaker console you'll find the entity you created in this notebook. If it was successfully created and validated, you should be able to select the entity and choose **Publish new ML Marketplace listing**.\n",
    "\n",
    "<img src=\"images/publish-to-marketplace-action.png\"/>\n",
    "\n",
    "You will be redirected to the [AWS Marketplace Management portal](https://aws.amazon.com/marketplace/management/ml-products/) where you will be able to build a listing.\n",
    "\n",
    "<img src=\"images/listing.png\"/>\n",
    "\n",
    "1. If your model targets multiple hardware types, remember to add each ModelPackage to the listing as separate versions.\n",
    "2. Click Add and fill in the model information. Kindly set Product visibility must be set to `Public`.\n",
    "<img src=\"images/public.png\"/>\n",
    "\n",
    "3. Allowlist account `171503325295`, `572320329544` and `559110549532` for access to the model. \n",
    "For region support select: `us-east-1, us-west-2, eu-west-1, eu-central-1, eu-west-2, ap-northeast-1, ap-south-1, ca-central-1, us-east-2, ap-northeast-2`\n",
    "<img src=\"images/allowlist-accs.png\"/>\n",
    "\n",
    "4. Under Pricing and terms, set pricing model as:\n",
    "**Inference based pricing (custom metering) at $0**\n",
    "\n",
    "You will see the following:\n",
    "(Optional) If the container did not implement the below please confirm and move forward. \n",
    "```\n",
    "I confirm that my model package supports the response header for custom metering. Example response header: X-Amzn-Inference-Metering:\n",
    "{\"Dimension\": \"inference.count\", \"ConsumedUnits\": 3}\n",
    "I understand that in absence of this header, default metering will be used instead.\n",
    "```\n",
    "\n",
    "<img src=\"images/inference-based-pricing.png\"/>\n",
    "\n",
    "5. Listing status should show as follows:\n",
    "**Do not click Sign off and publish**\n",
    "\n",
    "<img src=\"images/status-1.png\"/>\n",
    "\n",
    "6. Vissibility status of the listing should be `Limited`.\n",
    "\n",
    "<img src=\"images/status-2.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "* [Publishing your product in AWS Marketplace](https://docs.aws.amazon.com/marketplace/latest/userguide/ml-publishing-your-product-in-aws-marketplace.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
